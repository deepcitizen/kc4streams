= KC4 Streams (Kafka Commons for Streams)

== Introduction

**KC4 Streams** is a Java library that provides commons and reusable classes for building Kafka Streams applications.

== Built With

KC4 Streams is built with the following dependencies:

* Java 17
* Kafka Streams (>=3.1.x)

== Getting Started ?

Add the following dependency to your project :

Maven::
[source,xml]
----
<dependency>
    <groupId>io.streamthoughts</groupId>
    <artifactId>kc4streams</artifactId>
    <scope>${kc4streams.version}</scope>
</dependency>
----

Gradle::
[source]
----
implementation group: 'io.streamthoughts', name: 'kc4streams', version: '1.0.0'
----

== Usage ?

=== SafeDeserializer

This library provides a `SafeDeserializer` that can be used to wrap an existing `Deserializer`.
and catch any exception thrown during deserialization for returning a record called a sentinel-object
that you filter later in the Topology (e.g null, "N/A", -1, etc).

==== Creating a SafeDeserializer

[source,bash]
----
SafeDeserializer deserializer = new SafeDeserializer<>(
    new GenericAvroSerde().deserializer(), // the delegating deserializer
    (GenericRecord)null     			   // the sentinel-object to return when an exception is catch
);
----

==== Configuring a SafeDeserializer

The sentinel-object to return can also be configured.

[source,java]
----
SafeDeserializer<Double> deserializer = new SafeDeserializer<>(
    Serdes.Double().deserializer(), // the delegating deserializer
    Double.class    		        // the value type
);

Map<String, Object> configs = new HashMap<>();
configs.put(SafeDeserializerConfig.SAFE_DESERIALIZER_DEFAULT_VALUE_CONFIG, 0.0);
deserializer.configure(configs, false);
----

==== SafeSerde

The `SafeSerde` is a utility class allowing you to wrap existing Serde or Deserializer.

Behind the scene, `SafeSerde` uses the `SafeDeserializer` for wrapping existing Deserializer.

[source,java]
----
Serde<String> stringSerde = SafeSerdes.Double();
// or
SafeSerdes.serdeFrom(Serdes.String(), 0.0);
----

=== Dead Letter Topic

This library provides the `DeadLetterTopicExceptionHandler` class that implements, the `DeserializationExceptionHandler` allowing to send corrupted records into a dedicated topic.
By default, the `DeadLetterTopicExceptionHandler` will send corrupted records to a sink topic named based on the source topic - i.e: <source_topic_name>-rejected`.

In addition, the DeadLetterTopicExceptionHandler will enrich corrupted records with headers to help to investigate the cause of the exception.

==== GlobalDeadLetterTopicCollector

The `GlobalDeadLetterTopicCollector` utility class can be used for sending corrupted record to a dedicated topic (i.e., a Dead-Letter-Topic)
with contextual information about the error.

Configuration::
,===
Property,Description,Type,Default
`exception.handler.dlq.global.producer.<config>`,,,
`exception.handler.dlq.global.admin.<config>`,,,
`exception.handler.dlq.topics.auto.create.enabled`, Specifies whether missing DLQ topics should be automatically created.,,,
`exception.handler.dlq.topics.partitions`, Specifies the number of partitions to be used for DLQ topics.,,,
`exception.handler.dlq.topics.replication.factors`, Specifies the replication factor to be used for DLQ topics.,,,
,===

Usage::
[source, java]
----
// Create KafkaStreams client configuration
Map<String, Object> streamsConfigs = new HashMap<>();

// Initialize the GlobalDeadLetterTopicCollector.
GlobalDeadLetterTopicCollector.getOrCreate(streamsConfigs);

// Create a Kafka Stream Topology
StreamsBuilder streamsBuilder = new StreamsBuilder();
KStream<String, String> stream = streamsBuilder.stream(INPUT_TOPIC);
stream.mapValues((key, value) -> {
    Long output = null;
    try {
        output = Long.parseLong(value);
    } catch (Exception e) {
        // Sends the corrupted-record to a DLQ
        GlobalDeadLetterTopicCollector.get().send(
                INPUT_TOPIC + "-DLQ",
                key,
                value,
                Serdes.String().serializer(),
                Serdes.String().serializer(),
                Failed.withProcessingError((String) streamsConfigs.get(StreamsConfig.APPLICATION_ID_CONFIG), e)
        );
    }
    return output;
});
----


==== DeadLetterTopicExceptionHandler

Common configurations::
,===
Property,Description,Type,Default
`exception.handler.dlq.default.topic.extractor`, Specifies the fully-classified name of the class to be used for extracting the name of dead-letter topic, `class`, `io.streamthoughts.kc4streams.error.DefaultDeadLetterTopicNameExtractor`
`exception.handler.dlq.default.reponse`, The default response that must be returned by the handler [FAIL|CONTINUE],`string`,
`exception.handler.dlq.default.fail.errors`,Specifies the comma-separated list of FQCN of the exceptions on which the handler must fail,`List`,
`exception.handler.dlq.default.continue.errors`,Specifies the comma-separated list of FQCN of the exceptions on which the handler must continue.,,
`exception.handler.dlq.default.headers.<key>`,Specifies the value of a custom record-header to be added to the corrupted record send into the dead-letter topic.,`string`,
,===

Specific config properties for `ProducerExceptionHandler`

Specific config properties for `StreamsUncaughtExceptionHandler`




=== RocksDB

==== How to tune internal RocksDB state stores ?

KafkaStreams relies on RocksDB an embedded key-value store to provided persistent storage. Depending on the throughput of your application, you may want to tune internal RocksDB instances.
Kafka Streams allows you to customize the RocksDB settings for a given Store by implementing the interface `org.apache.kafka.streams.state.RocksDBConfigSetter`.

The custom implementation must then be configured using :

[source,java]
----
streamsConfig.put(StreamsConfig.ROCKSDB_CONFIG_SETTER_CLASS_CONFIG, CustomRocksDBConfig.class);
----

**KC4 Streams** provides a built-in `io.streamthoughts.kc4.streams.rocksdb.StreamsRocksDBConfigSetter` that allows
overriding not only some default RocksDB options but also to enable log statistics, for performance debugging, and shared memory usage.

Configuration::

,===
Property,Description,Type,Default
`rocksdb.stats.enable`,Enable RocksDB statistics,`boolean`,-
`rocksdb.stats.dump.period.sec`,Specifies the RocksDB statistics dump period in seconds.,`integer`,-
`rocksdb.log.dir`,Specifies the RocksDB log directory,`string,
`rocksdb.log.level`,Specifies the RocksDB log level (see org.rocksdb.InfoLogLevel).,`string`,-
`rocksdb.log.max.file.size`,Specifies the RocksDB maximum log file size.,`integer`,-
`rocksdb.max.write.buffer.number`,Specifies the maximum number of memtables build up in memory before they flush to SST files.,`integer`,
`rocksdb.write.buffer.size`,Specifies the size of a single memtable.,`long`,-
`rocksdb.memory.managed`,Enable automatic memory management across all RocksDB instances.,`boolean`,`false`
`rocksdb.memory.write.buffer.ratio`,Specifies the ratio of total cache memory which will be reserved for write buffer manager. This property is only used when `rocksdb.memory.managed` is set to true.,`double`,`0.5`
`rocksdb.memory.high.prio.pool.ratio`,Specifies the ratio of cache memory that is reserved for high priority blocks (e.g.: indexes filters and compressions blocks).,`double`,`0.1`
`rocksdb.memory.strict.capacity.limit`,Create a block cache with strict capacity limit (i.e. insert to the cache will fail when cache is full). This property is only used when `rocksdb.memory.managed` is set to true or `rocksdb.block.cache.size` is set.,`boolean`,`false`
`rocksdb.block.cache.size`,Specifies the total size to be used for caching uncompressed data blocks.,`long`,`false`
`rocksdb.compaction.style`,Specifies the compaction style.,`string`,-
`rocksdb.compression.type`,Specifies the compression type.`string`,-
`rocksdb.files.open`,Specifies the maximum number of open files that can be used per RocksDB instance.,`long`,-
`rocksdb.max.background.jobs`,Specifies the maximum number of concurrent background jobs (both flushes and compactions combined).,`integer`,-
,===

Example::
[source,java]
----
var streamsConfig = new HashMap<String, Object>();
streamsConfig.put(StreamsConfig.ROCKSDB_CONFIG_SETTER_CLASS_CONFIG, StreamsRocksDBConfigSetter.class);
streamsConfig.put(RocksDBConfig.ROCKSDB_MEMORY_MANAGED_CONFIG, true);
streamsConfig.put(RocksDBConfig.ROCKSDB_STATS_ENABLE_CONFIG, true);
streamsConfig.put(RocksDBConfig.ROCKSDB_LOG_DIR_CONFIG, "/tmp/rocksdb-logs");
----

NOTE: Please read the official documentation for more information: https://github.com/facebook/rocksdb/wiki/RocksDB-Tuning-Guide[RocksDB Tuning Guide]

=== StateListener

== Contribute to KC4 Streams

== Licence
Copyright 2022 StreamThoughts.

Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License